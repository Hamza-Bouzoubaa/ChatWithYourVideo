{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d193a6-ef7e-4bf6-bdd1-5710e8de0ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "C:\\Users\\hamza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_audiomentations\\utils\\io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "torchvision is not available - cannot save figures\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "from pyannote.audio import Pipeline\n",
    "from pydub import AudioSegment  \n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('hf_token')\n",
    "\n",
    "hf_token = \"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\"\n",
    "\n",
    "\n",
    "\n",
    "def transcribe_with_diarization(audio_file_path,OutputFile, model_type=\"large-v2\", device=\"cpu\", hf_token=hf_token):\n",
    "    # Step 1: Load the WhisperX model with float32 compute type\n",
    "    model = whisperx.load_model(model_type, device, compute_type=\"float32\")\n",
    "\n",
    "    # Step 2: Load the audio file\n",
    "    audio = whisperx.load_audio(audio_file_path)\n",
    "\n",
    "    # Step 3: Transcribe audio\n",
    "    result = model.transcribe(audio)\n",
    "\n",
    "    # Step 4: Load an alignment model for diarization\n",
    "    align_model, align_metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "\n",
    "    # Step 5: Align Whisper output\n",
    "    result = whisperx.align(result[\"segments\"], align_model, align_metadata, audio, device, return_char_alignments=False)\n",
    "\n",
    "    # Step 6: Perform speaker diarization\n",
    "    diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
    "    diarize_segments = diarize_model(audio)\n",
    "\n",
    "    # Step 7: Assign speaker IDs to word-level segments\n",
    "    result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "\n",
    "    transcription = result[\"segments\"]\n",
    "    \n",
    "    text = transcription[0][\"speaker\"]+\": \\n\" \n",
    "    for i in range(len(transcription)):\n",
    "    \n",
    "    \n",
    "    \n",
    "        if (i>0  and (transcription[i-1][\"speaker\"] != transcription[i][\"speaker\"])):\n",
    "            text += \"\\n\\n\"+transcription[i][\"speaker\"]+\": \\n\"\n",
    "        text  += transcription[i][\"text\"]\n",
    "    \n",
    "      \n",
    "    # Write the text to the file\n",
    "    with open(OutputFile, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "    \n",
    "    print(f\"Text has been saved to {OutputFile}\")\n",
    "\n",
    "\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcb41985-9502-4e4a-950e-03b0355eee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\hamza\\.cache\\torch\\whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.1.1+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m transcription \u001b[38;5;241m=\u001b[39m  \u001b[43mtranscribe_with_diarization\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAudio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#print(transcription)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m, in \u001b[0;36mtranscribe_with_diarization\u001b[1;34m(audio_file_path, OutputFile, model_type, device, hf_token)\u001b[0m\n\u001b[0;32m     22\u001b[0m audio \u001b[38;5;241m=\u001b[39m whisperx\u001b[38;5;241m.\u001b[39mload_audio(audio_file_path)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Step 3: Transcribe audio\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Step 4: Load an alignment model for diarization\u001b[39;00m\n\u001b[0;32m     28\u001b[0m align_model, align_metadata \u001b[38;5;241m=\u001b[39m whisperx\u001b[38;5;241m.\u001b[39mload_align_model(language_code\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\whisperx\\asr.py:194\u001b[0m, in \u001b[0;36mFasterWhisperPipeline.transcribe\u001b[1;34m(self, audio, batch_size, num_workers, language, task, chunk_size, print_progress, combined_progress)\u001b[0m\n\u001b[0;32m    187\u001b[0m vad_segments \u001b[38;5;241m=\u001b[39m merge_chunks(\n\u001b[0;32m    188\u001b[0m     vad_segments,\n\u001b[0;32m    189\u001b[0m     chunk_size,\n\u001b[0;32m    190\u001b[0m     onset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vad_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvad_onset\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    191\u001b[0m     offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vad_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvad_offset\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    192\u001b[0m )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 194\u001b[0m     language \u001b[38;5;241m=\u001b[39m language \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m     task \u001b[38;5;241m=\u001b[39m task \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m faster_whisper\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mTokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhf_tokenizer,\n\u001b[0;32m    197\u001b[0m                                                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mis_multilingual, task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    198\u001b[0m                                                         language\u001b[38;5;241m=\u001b[39mlanguage)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\whisperx\\asr.py:252\u001b[0m, in \u001b[0;36mFasterWhisperPipeline.detect_language\u001b[1;34m(self, audio)\u001b[0m\n\u001b[0;32m    248\u001b[0m model_n_mels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfeat_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m segment \u001b[38;5;241m=\u001b[39m log_mel_spectrogram(audio[: N_SAMPLES],\n\u001b[0;32m    250\u001b[0m                               n_mels\u001b[38;5;241m=\u001b[39mmodel_n_mels \u001b[38;5;28;01mif\u001b[39;00m model_n_mels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m80\u001b[39m,\n\u001b[0;32m    251\u001b[0m                               padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m audio\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m N_SAMPLES \u001b[38;5;28;01melse\u001b[39;00m N_SAMPLES \u001b[38;5;241m-\u001b[39m audio\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 252\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdetect_language(encoder_output)\n\u001b[0;32m    254\u001b[0m language_token, language_probability \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\whisperx\\asr.py:86\u001b[0m, in \u001b[0;36mWhisperModel.encode\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     83\u001b[0m     features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(features, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     84\u001b[0m features \u001b[38;5;241m=\u001b[39m faster_whisper\u001b[38;5;241m.\u001b[39mtranscribe\u001b[38;5;241m.\u001b[39mget_ctranslate2_storage(features)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_cpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_cpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transcription =  transcribe_with_diarization(\"Audio.wav\",\"fle\")\n",
    "\n",
    "#print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d4c3752-2843-4fc8-9d54-40d81cb3ee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text has been saved to transcrption.txt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7019fd6-4dad-44b2-b4c9-8813c790c7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SPEAKER_02said: \\n What's really important is that we think how to store data for the long term and how we store it sustainably.  If we carry on the way we're going, we're going to have to concrete the whole planet just to store the data that we're generating. And of course, that's not acceptable. The hard disk drives and tape are used to store the large amounts of data, those photos that you take on your phone. Now, that is all magnetic technology. It all has a finite lifetime. You therefore have to keep copying it over to new generations of media. A hard disk drive might last five years. A tape, well, if you're brave, it might last 10 years.  But once that lifetime is up, you've got to copy it over. And that, frankly, is both difficult and it's also tremendously unsustainable. If you think of all that energy, all that resource that we're using, glass is actually very durable. You can put it in boiling water. You can put it in an oven. You can even scratch the outside of it. And it's going to be able to keep that data that's stored inside it completely safe. \\nSPEAKER_01\\n So in Project Silica, we're developing a completely new storage technology, and we're designing it for the cloud. And what this is going to enable is a completely new way of storing data practically forever. \\nSPEAKER_05\\nSo there are four different aspects to the project. \\nSPEAKER_00\\n Welcome to the Bright Lab. This is the first step of the data into the glass. So we create these very short laser pulses with our laser systems that allow us to modify the glass and to store the data in it. \\nSPEAKER_05\\nSo the data is actually encoded in something we describe as voxels. Now you're probably familiar with what a pixel is, and a voxel is just a three-dimensional pixel.  This is where actually our data in the glass is read. We have a live feed here of how the boxes look like once we find them. So a reader is really a computer-controlled, very high-speed, very accurate microscope. And we can move the piece of glass very rapidly to the area where it contains the data we actually want to read. \\nSPEAKER_03\\n Now we're in the stage three of the glass journey, which is where we process the information that has been written in the glass so that we can decode the symbols. \\nSPEAKER_04\\nSo once the glass is written, it's read, and it's decoded, it comes here to the library. When someone needs their data back, the Silica system will send one of these robots to pick the piece of glass that their data was written on and bring it to a reader.  A really exciting aspect of this design is all the complexity is in the robot. Where the rest of the library, it's passive, there's no electricity in any of the storage units, and it's a much more sustainable way of us storing our data for the future. \\nSPEAKER_05\\n The ability to write data and know that it's going to remain. It won't get changed, it won't get lost, and it won't cost the earth. \\nSPEAKER_02\\nSustainability is so important to both Microsoft and the planet. That's why I joined Microsoft 25 years ago, and that's still what keeps me coming in today. \""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e36b7-a022-4255-8fa8-7eb1c96a4f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
